{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "861e8a7c-2737-4a07-a422-03257b857b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8e9c1df-6610-40a7-980e-d29a34932ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1.   Pair up with a partner. Assign one person as the interviewer and the other as the candidate\\n 2.   Interviewer – take about 5 minutes to read through the case in your head. Then read the prompt to the\\n      candidate\\n 3.   Candidate – Note on paper important details from the prompt. Ask clarifying questions, then ask the\\n      interviewer if you can take about 1 minute to draw up a structured problem-solving approach. Then talk\\n      through your structure with the interviewer and ask for any additional information you think might be helpful\\n 4.   Interviewer – Listen carefully to candidate’s structure and logic. Are there any crucial pieces he/she is\\n      missing? Is he/she going down the right track? If not, try to lead the candidate in the right direction. Provide\\n      additional information only when the candidate asks for them. Then go through the questions one by one,\\n      providing the exhibits as appropriate' metadata={'source': 'new_articles/Yale_YGCC_Life_Sciences_Casebook_2014_1.txt'}\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('./new_articles/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(texts[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56c719f3-a4ff-48fd-9bc9-13213b0ed2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = [doc.metadata for doc in texts]\n",
    "text_content_list = [doc.page_content for doc in texts]\n",
    "id_list=id_list = [\"doc\" + str(i + 1) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d72ed8f-896f-4005-b811-cf7cc9ef7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Rag:\n",
    "    def __init__(self, embedding_name, llm):\n",
    "        self.embedding_name = embedding_name\n",
    "        self.llm = llm\n",
    "    def get_embedding_name(self):\n",
    "        return self.embedding_name\n",
    "\n",
    "    def get_llm(self):\n",
    "        return self.llm  \n",
    "    def generate_embedding(self):\n",
    "        load_dotenv()\n",
    "        if self.embedding_name == \"GoogleGenAI Embedding\":\n",
    "            google_API_KEY= os.getenv(\"GOOGLE_API_KEY\")\n",
    "            embedding_function= embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=google_API_KEY)\n",
    "            return embedding_function\n",
    "        elif self.embedding_name == \"OpenAI Embedding\":\n",
    "            from openai import OpenAI\n",
    "            OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")\n",
    "            embedding_function = embedding_functions.OpenAIEmbeddingFunction(\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            model_name=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            return embedding_function\n",
    "            \n",
    "        elif self.embedding_name == \"jinaAi Embedding\":\n",
    "            jina_api= os.getenv(\"Jina_Api\")\n",
    "            jinaai_ef = embedding_functions.JinaEmbeddingFunction(\n",
    "                api_key=jina_api,\n",
    "                model_name=\"jina-embeddings-v2-base-en\"\n",
    "            )\n",
    "            \n",
    "            return embedding_function\n",
    "        else:\n",
    "            return None \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bb21574-7b3c-4c71-88b8-ac4f4d42c66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997477bd4278478383e58be7128c5c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select Embedding:', options=('GoogleGenAI Embedding', 'OpenAI Embedding', 'jinaAi Embedd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf2b6f1c9044ebe84e108dd9a26ff23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select LLm:', options=('Gemini', 'OpenAI'), value='Gemini')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "embd_options = [\"GoogleGenAI Embedding\", \"OpenAI Embedding\", \"jinaAi Embedding\"]\n",
    "dropdown_1 = widgets.Dropdown(options=embd_options, description='Select Embedding:')\n",
    "\n",
    "llm_options = [\"Gemini\", \"OpenAI\"]\n",
    "dropdown_2 = widgets.Dropdown(options=llm_options, description='Select LLm:')\n",
    "\n",
    "selected_embedding = None  \n",
    "selected_llm = None \n",
    "\n",
    "def on_dropdowns_change(change):\n",
    "    global selected_embedding, selected_llm\n",
    "    selected_embedding = dropdown_1.value\n",
    "    selected_llm = dropdown_2.value\n",
    "\n",
    "def display_dropdowns():\n",
    "    display(dropdown_1)\n",
    "    display(dropdown_2)\n",
    "\n",
    "def access_selected_values():\n",
    "    global selected_embedding, selected_llm\n",
    "    return selected_embedding, selected_llm\n",
    "\n",
    "dropdown_1.observe(on_dropdowns_change, names='value')\n",
    "dropdown_2.observe(on_dropdowns_change, names='value')\n",
    "\n",
    "# Set default values for dropdowns\n",
    "dropdown_1.value = embd_options[0]\n",
    "dropdown_2.value = llm_options[0]\n",
    "\n",
    "display_dropdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8716b55-d94f-4e13-97b4-f9cef64669f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding data: GoogleGenAI Embedding\n",
      "LLm data: Gemini\n"
     ]
    }
   ],
   "source": [
    "embd, llm=access_selected_values()\n",
    "rag_instance = Rag(embd, llm)\n",
    "print(\"Embedding data:\", rag_instance.get_embedding_name())\n",
    "print(\"LLm data:\", rag_instance.get_llm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40d5176b-649c-4440-baed-32cc090c35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=rag_instance.get_llm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a869a5e-42b1-412a-b3b0-71ec09bb0bd4",
   "metadata": {},
   "source": [
    "ChromaDB Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57f87b53-3d7e-43a2-9d86-b20119f98fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8414aeb3-5981-443f-b4da-77282056ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Existing collection 'Articles' deleted.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"Articles\"\n",
    "existing_collections = [collection.name for collection in chroma_client.list_collections()]\n",
    "\n",
    "if collection_name in existing_collections:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "    print(f\"Info: Existing collection '{collection_name}' deleted.\")\n",
    "else:\n",
    "    print(\"No collection found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc57522-0a72-434a-8ebd-8e3e721d7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_store = chroma_client.get_or_create_collection(name=\"Articles\",\n",
    "                                                      embedding_function=rag_instance.generate_embedding())\n",
    "vector_store.add(ids=id_list, documents=text_content_list,metadatas=metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a072f363-d74c-43dd-b94e-7bd9601a6dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc81', 'doc71']], 'distances': [[0.48240000009536743, 0.5079999566078186]], 'metadatas': [[{'source': 'new_articles/Yale_YGCC_Life_Sciences_Casebook_2014_1.txt'}, {'source': 'new_articles/Yale_YGCC_Life_Sciences_Casebook_2014_1.txt'}]], 'embeddings': None, 'documents': [['Market Penetration\\nPercent of Revenue\\n\\n\\n\\n\\n                                                  Net Profit                          35%\\n                     70%                65%       Distribution                        30%\\n                     60%                          Production                          25%\\n                              30%\\n                     50%                          R&D                                 20%\\n\\n                     40%                                                              15%', 'Market                                                                                                      Competitive                 Culture\\n         for Drug                  Profitability                      Synergies                          Intangible    Response     Regulation\\n                                                                                                                                                 Difference\\n\\n\\n                                                              Cost           Revenue         Brand                    Expertise\\nCompetition     Customers    Revenue           Costs        Reduction        Increase        Name        Patents\\n\\n\\n\\n                                  Production           Distribution']], 'uris': None, 'data': None}\n"
     ]
    }
   ],
   "source": [
    "# results = vector_store.query(\n",
    "#         query_texts=\"What is the amount of floss used in America\",\n",
    "#         n_results=2\n",
    "#     )\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50862d9-bbef-44dd-83d0-89f778db59ec",
   "metadata": {},
   "source": [
    "Evaluation BY True_Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f2cf669-0b5b-42d4-a11a-0b5b57821512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39a0e672-f5e8-4c3e-8f88-d026228d7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "tru = Tru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c970a6d0-8fed-42ca-bc2b-398b722f86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")\n",
    "class RAG_from_scratch:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = vector_store.query(\n",
    "        query_texts=query,\n",
    "        n_results=2\n",
    "    )\n",
    "        return results['documents'][0]\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        if llm==\"Gemini\":\n",
    "            import google.generativeai as genai\n",
    "            message = [{\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    f\"We have provided context information below. \\n\"\n",
    "                    f\"---------------------\\n\"\n",
    "                    f\"{context_str}\"\n",
    "                    f\"\\n---------------------\\n\"\n",
    "                    f\"Given this information, please answer the question: {query}\"\n",
    "                ]\n",
    "            }]\n",
    "            model = genai.GenerativeModel('gemini-pro')\n",
    "            response = model.generate_content(message)\n",
    "            response_txt = response.text\n",
    "            return response_txt\n",
    "\n",
    "        elif llm==\"OpenAI\":\n",
    "            import openai\n",
    "            openai.api_key = OPENAI_API_KEY\n",
    "            completion = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            temperature=0,\n",
    "            messages=\n",
    "            [\n",
    "                {\"role\": \"user\",\n",
    "                \"content\": \n",
    "                f\"We have provided context information below. \\n\"\n",
    "                f\"---------------------\\n\"\n",
    "                f\"{context_str}\"\n",
    "                f\"\\n---------------------\\n\"\n",
    "                f\"Given this information, please answer the question: {query}\"\n",
    "                }\n",
    "            ]\n",
    "            ).choices[0].message.content\n",
    "            return completion\n",
    "        else:\n",
    "            print(\"no llm\")\n",
    "    \n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        completion = self.generate_completion(query, context_str)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4bb8e2f-cf33-498e-ae0a-f116b912e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_instance = RAG_from_scratch()\n",
    "# my_query = \"how to manage project\"\n",
    "# result = rag_instance.query(my_query)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2db115fe-61b9-45f3-afcb-7d9369c0398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retrieve.rets.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Answer Relevance, input prompt will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input question will be set to __record__.app.retrieve.args.query .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.retrieve.rets.collect() .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback, Select\n",
    "from trulens_eval.feedback import Groundedness\n",
    "from trulens_eval.feedback.provider.openai import OpenAI as fOpenAI\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize provider class\n",
    "fopenai = fOpenAI()\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=fopenai)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_qa_relevance = (\n",
    "    Feedback(fopenai.relevance_with_cot_reasons, name = \"Answer Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(fopenai.qs_relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on(Select.RecordCalls.retrieve.args.query)\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb010c20-e4b6-42d1-ad46-4d9ae2672f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag,\n",
    "    app_id = 'RAG v3',\n",
    "    feedbacks = [f_groundedness, f_qa_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbc2cca1-bd38-4b40-83be-76115e7db98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(\"What is the amount of floss used in America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d986c643-76cc-4ff9-9379-9d26ba1afd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a25deb7a5cb46549968b30b89eb9465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.1.10:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7de71b-a6ed-4782-ae89-260417352fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
